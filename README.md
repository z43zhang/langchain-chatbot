# ðŸŒ **Live Demo**: [Click here to try it out](https://zhang-langchain-chatbot.streamlit.app/)

![App Preview]()

### ðŸ“’ An interactive Retrieval-Augmented Generation (RAG) chatbot that answers questions about [LangChain's official documentation](https://python.langchain.com/). Powered by **OpenAI GPT-4o**, **Pinecone**, and **LangChain**, with a **Streamlit** UI for live chat experience.

---

# ðŸš€ Features

* ðŸ” **LangChain RAG Pipeline** â€” Built a Retrieval-Augmented Generation system using `HistoryAwareRetriever`, `StuffDocumentsChain`, and `RetrievalChain` from LangChain
* ðŸ—‚ï¸ **Custom Document Ingestion** â€” Scraped LangChain doc pages using Firecrawl API, converted to Markdown/HTML, and chunked via `RecursiveCharacterTextSplitter`
* ðŸ§  **Semantic Search Index** â€” Embedded 2,383 text chunks with OpenAIâ€™s `text-embedding-3-small` (1536-dim) and stored in a Pinecone index using cosine similarity
* ðŸ§¬ **Deterministic Chunk IDs** â€” Prevented duplicate uploads using UUIDv5 generated from MD5 hashes of content
* ðŸ”„ **Multi-Turn Query Handling** â€” Rephrased follow-up questions using chat context via `chat-langchain-rephrase` prompt and `HistoryAwareRetriever`
* ðŸ¤– **Grounded Generation** â€” Used GPT-4o-mini to answer questions strictly based on retrieved document chunks using `retrieval-qa-chat` prompt
* ðŸ–¥ï¸ **Interactive Chat App** â€” Streamlit-based UI with avatars, timestamps, real-time responses, reset functionality, and chat history download
* ðŸ”— **Source Attribution** â€” Each response cites original documentation URLs used during retrieval for full transparency

---

# ðŸ› ï¸ Tech Stack

| Layer            | Tools / Libraries                                     |
|------------------|--------------------------------------------------------|
| UI               | Streamlit                                              |
| LLM              | OpenAI `gpt-4o-mini`                                   |
| Embeddings       | OpenAI `text-embedding-3-small`                        |
| Vector Store     | Pinecone                                               |
| RAG Framework    | LangChain (Retriever, Chains, Prompt Hub)              |
| Web Crawling     | Firecrawl API                                          |
| Chunking         | `RecursiveCharacterTextSplitter`                      |

---

# ðŸ§ª Project Breakdown

This section breaks down the project **based strictly on the sequence and logic flow from the three Python files**: `ingestion.py`, `core.py`, and `main.py`. It follows the full life cycle: data ingestion â†’ RAG pipeline â†’ UI deployment.

## 1. ðŸ“¥ Ingest Documentation 

The ingestion pipeline scrapes and prepares LangChain docs for semantic search:

- **Data Source**: LangChain documentation URLs
- **Crawling Tool**: `FirecrawlApp().crawl_url()`
  - Supports both Markdown and HTML output formats
- **Chunking**: 
  - Tool: `RecursiveCharacterTextSplitter`  
  - Config: `chunk_size=800`, `chunk_overlap=100`
- **UUID Hashing**: 
  - Generates deterministic chunk IDs using `uuid5(NAMESPACE_URL, MD5(content))`  
  - Prevents duplicates during repeated runs
- **Embedding**: 
  - Model: `text-embedding-3-small` (OpenAI)
  - Dimensions: 1536
- **Storage**: 
  - Vector store: `PineconeVectorStore`
  - Record Count: `2383`

> âœ… Output: A Pinecone index filled with chunked and deduplicated documentation vectors.

---

## 2. ðŸ”§ RAG Pipeline

This module handles backend processing for user queries using LangChainâ€™s chain API.

### ðŸ”¹ Retriever
   - Wraps Pineconeâ€™s retriever in a `HistoryAwareRetriever`  
   - Uses the prompt `chat-langchain-rephrase` from LangChain Hub to convert follow-up queries into standalone ones

### ðŸ”¹ Document Combiner  
   - Uses `StuffDocumentsChain` with `retrieval-qa-chat` prompt  
   - Merges retrieved documents + user query â†’ formatted prompt for LLM

### ðŸ”¹ Retrieval Chain
   - Uses `create_retrieval_chain()` to link retriever + combiner  
   - Input: `query`, `chat_history`  
   - Output: `result["answer"]`, `result["context"]`

> âœ… Output: A structured dictionary containing the final answer and source documents.

---

## 3. ðŸ’¬ Streamlit App 

This is the front-end entry point that drives user interaction.

- `st.session_state` persist:
  - User inputs
  - Assistant responses
  - Chat history for multi-turn context
  - Timestamps for each message
- Captures input from `st.chat_input()`
- Records user timestamp
- Sends input to `run_llm()` (calls backend pipeline)
- Formats output:
  - Appends sources via `create_sources_string()`
  - Adds assistant timestamp
- Displays chat messages with avatars (`ðŸ§‘â€ðŸ’»` for user, `ðŸ¤–` for assistant)
- Provides:
  - ðŸ”„ Reset chat button (clears session state)
  - ðŸ–¨ï¸ Download chat history as `.txt`

> âœ… Output: A live conversational UI backed by a real-time RAG pipeline with full session memory and source transparency.

---





